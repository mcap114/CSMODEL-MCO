{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92be349",
   "metadata": {},
   "source": [
    "# CSMODEL Project - Netflix Userbase Dataset Case Study\n",
    "\n",
    "### Group 8\n",
    "CAPAROS, MIGUEL ANTONIO <br> \n",
    "FERRER, ANGEL JUNE <br>\n",
    "MARTINEZ, AZELIAH <br>\n",
    "VILLANUEVA, KEISHA LEIGH <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e871d42",
   "metadata": {},
   "source": [
    "# I. Dataset Description\n",
    "\n",
    "The Netflix Userbase Dataset provides a snapshot of a sample Netflix userbase, showcasing various aspects of user subscriptions, revenue, account details, and activity. Each row represents a unique user, identified by their User ID. The dataset serves as a synthetic representation and does not reflect actual Netflix user data. \n",
    "\n",
    "\n",
    "## Data Collection Process\n",
    "\n",
    "The dataset is synthetically sourced, and as such, any conclusions and insights may not accurately reflect real-world data. \n",
    "\n",
    "\n",
    "\n",
    "## Dataset File Structure\n",
    "\n",
    "Each row in the dataset represents a unique user. Each columns contain various details about the user. The dataset contains a total of 2500 observations (rows) and 10 variables (columns). Each variable provides specific details about the users, enabling analysis of subscription patterns, revenue generation, and user behavior.\n",
    "\n",
    "***If the dataset is composed of different files that you will combine in the succeeding steps, describe the structure and the contents of each file.***\n",
    "\n",
    "\n",
    "## Dataset Variables\n",
    "\n",
    "The dataset contains 10 variables, each representing different user information such as:\n",
    "\n",
    "- User ID: A unique identifier for each user.\n",
    "- Subscription Type: The type of subscription the user has (basic, standard, or premium).\n",
    "- Monthly Revenue: The monthly revenue generated from the user's subscription.\n",
    "- Join Date: The date the user joined Netflix.\n",
    "- Last Payment Date: The date of the user's last payment.\n",
    "- Country: The country where the user is located.\n",
    "- Age: The age of the user.\n",
    "- Gender: The gender of the user.\n",
    "- Device Type: The type of device the user primarily uses to access Netflix (e.g., Smart TV, Mobile, Desktop, Tablet).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e920be",
   "metadata": {},
   "source": [
    "# II. Data Cleaning\n",
    "\n",
    "In this section of the notebook, we will focus on cleaning the Netflix Userbase Dataset. Data cleaning is an essential step in the data analysis process, aimed at preparing raw data for further exploration and analysis. It involves identifying and correcting errors or inconsistencies in the data, handling missing values, removing duplicates, and ensuring data quality and integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21fed12",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "We begin by importing **`numpy`** and **`pandas`** which are essential libraries for data manipulation and analysis in Python to begin our data cleaning process.\n",
    "\n",
    "**`numpy`** is a fundamental package for numerical computing in Python, providing support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays. [**`pandas`**](https://pandas.pydata.org/pandas-docs/stable/index.html) is a Python software library that offers data structures and tools for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051b3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d88c0f",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a8d85",
   "metadata": {},
   "source": [
    "Insert description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f73be4",
   "metadata": {},
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "Our first step is to load the dataset using pandas, which will import the data into a pandas `DataFrame`. We use the [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) function to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "userbase_df = pd.read_csv('Netflix Userbase.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22031596",
   "metadata": {},
   "source": [
    "When loading a new dataset, it is advisable to utilize the [`info`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) function, as it displays general information regarding the dataset's structure and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db605c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "userbase_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482093d4",
   "metadata": {},
   "source": [
    "We will use the [`head`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) function to quickly view the first few rows of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "userbase_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244c015",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171e0b3",
   "metadata": {},
   "source": [
    "Detecting and managing missing values is crucial for data analysis. To identify missing data within our DataFrame, we will use the [`isnull`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html) function in combination with [`sum`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html). This approach allows us to understand the extent of missing values in each column, facilitating appropriate strategies for data cleaning and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9530fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = userbase_df.isnull().sum()\n",
    "print(\"Missing data:\\n\", missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a8c65",
   "metadata": {},
   "source": [
    "## Outlier Handling\n",
    "\n",
    "Insert description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dbd664",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Insert description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0ea9b",
   "metadata": {},
   "source": [
    "## Duplicate Handling\n",
    "\n",
    "Duplicates in datasets can impact the accuracy of analysis results and should be managed to ensure data integrity. Identifying and managing duplicates is an essential step in data preprocessing. We use the [`duplicated`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html) function to find rows that are exact duplicates based on specified columns (`subset`). For instance, we will check for duplicates based on 'Country' and 'Subscription Type'. If duplicates are identified (`not duplicate_rows.empty`), they are displayed for further examination. Otherwise, a message indicating no duplicates are found is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a7daf",
   "metadata": {},
   "source": [
    "This code snippet identifies and prints duplicate rows in the dataset that have the same values in the 'Country' column. The `duplicated()` function with `keep=False` ensures all occurrences of duplicates are highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57154859",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = userbase_df[userbase_df.duplicated(subset='Country', keep=False)]\n",
    "\n",
    "if not duplicate_rows.empty:\n",
    "    print(\"Duplicate rows based on 'Country':\")\n",
    "    print(duplicate_rows)\n",
    "else:\n",
    "    print(\"No duplicate rows found based on 'Country'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ee8b0",
   "metadata": {},
   "source": [
    "Similarly, this code checks for duplicates based on the 'Subscription Type' column. It provides a clear indication of whether duplicate records exist in the dataset related to subscription types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527cda8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = userbase_df[userbase_df.duplicated(subset='Subscription Type', keep=False)]\n",
    "\n",
    "if not duplicate_rows.empty:\n",
    "    print(\"Duplicate rows based on 'Subscription Type':\")\n",
    "    print(duplicate_rows)\n",
    "else:\n",
    "    print(\"No duplicate rows found based on 'Subscription Type'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98517c8",
   "metadata": {},
   "source": [
    "## Handling Inconsistent Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c462a3",
   "metadata": {},
   "source": [
    "Inconsistent formatting in a dataset can cause errors in analysis and lead to misinterpreted results. Different representations of the same data, such as variations in text case ('United States', 'united States', 'UNITED STATES') or inconsistent date formats, can be mistakenly treated as different values. This can distort analysis outcomes and produce inaccurate conclusions. By standardizing data formats, we ensure uniformity, which allows for accurate comparisons and aggregations. This consistency is essential for reliable data analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237f88e",
   "metadata": {},
   "source": [
    "To address this, let's standardize the 'Country' and 'Subscription Type' columns to lowercase. Using [`str.lower`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.lower.html) converts all entries in the 'Country' and 'Subscription Type' columns to lowercase, ensuring consistency. The [`unique`](https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html) function is then used to display all unique values in the column after the transformation, allowing us to verify that the standardization was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07ac84fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values after converting 'Country' to lowercase:\n",
      "['united states' 'canada' 'united kingdom' 'australia' 'germany' 'france'\n",
      " 'brazil' 'mexico' 'spain' 'italy']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userbase_df['Country'] = userbase_df['Country'].str.lower()\n",
    "\n",
    "print(\"Unique values after converting 'Country' to lowercase:\")\n",
    "print(userbase_df['Country'].unique())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f325eb",
   "metadata": {},
   "source": [
    "Convert all subsription types to lowercase for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "803874d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values after converting 'Subscription Type' to lowercase:\n",
      "['basic' 'premium' 'standard']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userbase_df['Subscription Type'] = userbase_df['Subscription Type'].str.lower()\n",
    "\n",
    "print(\"Unique values after converting 'Subscription Type' to lowercase:\")\n",
    "print(userbase_df['Subscription Type'].unique())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266584b",
   "metadata": {},
   "source": [
    "# III. Exploratory Data Analysis\n",
    "\n",
    "In this section of the notebook, we aim to explore and understand various aspects of the Netflix Userbase Dataset through exploratory data analysis. We will address the following questions:\n",
    "\n",
    "1. **Subscription Description**\n",
    "\n",
    "\n",
    "   *Question: What is the distribution of users across different subscription types?*\n",
    "\n",
    "\n",
    "\n",
    "2. **Revenue Analysis**\n",
    "\n",
    "\n",
    "   *Question: How much revenue is generated from each subscription type?*\n",
    "\n",
    "\n",
    "3. **User Retention**\n",
    "\n",
    "\n",
    "   *Question:*\n",
    "\n",
    "\n",
    "4. **Device Type Usage**\n",
    "\n",
    "\n",
    "   *Question: What is the distribution of device types used by Netflix users?*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
